{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e57f53a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port', '0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "appName(\"Spark\").\\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "170ae4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:40903\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9395099f98>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc1e2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table if not exists itv009820_retaildb.employees(emp_id int,emp_name string,dept_id int, salary int)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc115bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th><th>dept_id</th><th>salary</th></tr>\n",
       "<tr><td>3</td><td>sai pavan</td><td>1</td><td>30000</td></tr>\n",
       "<tr><td>5</td><td>rajesh</td><td>2</td><td>16000</td></tr>\n",
       "<tr><td>4</td><td>nagesh</td><td>2</td><td>15000</td></tr>\n",
       "<tr><td>1</td><td>jeswanth</td><td>1</td><td>10000</td></tr>\n",
       "<tr><td>2</td><td>naidu</td><td>1</td><td>20000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+---------+-------+------+\n",
       "|emp_id| emp_name|dept_id|salary|\n",
       "+------+---------+-------+------+\n",
       "|     3|sai pavan|      1| 30000|\n",
       "|     5|   rajesh|      2| 16000|\n",
       "|     4|   nagesh|      2| 15000|\n",
       "|     1| jeswanth|      1| 10000|\n",
       "|     2|    naidu|      1| 20000|\n",
       "+------+---------+-------+------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from  itv009820_retaildb.employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783caa67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table itv009820_retaildb.departments(dept_id int, dept_name string)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8251b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dept_id</th><th>dept_name</th></tr>\n",
       "<tr><td>1</td><td>CSE</td></tr>\n",
       "<tr><td>2</td><td>ECE</td></tr>\n",
       "<tr><td>3</td><td>EEE</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+---------+\n",
       "|dept_id|dept_name|\n",
       "+-------+---------+\n",
       "|      1|      CSE|\n",
       "|      2|      ECE|\n",
       "|      3|      EEE|\n",
       "+-------+---------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from itv009820_retaildb.departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08a00fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"insert into itv009820_retaildb.employees values(2, 'naidu', 1, 20000)\")\n",
    "spark.sql(\"insert into itv009820_retaildb.employees values(3, 'sai pavan', 1, 30000)\")\n",
    "spark.sql(\"insert into itv009820_retaildb.employees values(4, 'nagesh', 2, 15000)\")\n",
    "spark.sql(\"insert into itv009820_retaildb.employees values(5, 'rajesh', 2, 16000)\")\n",
    "spark.sql(\"insert into itv009820_retaildb.departments values(1, 'CSE')\")\n",
    "spark.sql(\"insert into itv009820_retaildb.departments values(2, 'ECE')\")\n",
    "spark.sql(\"insert into itv009820_retaildb.departments values(3, 'EEE')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b789fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"insert into itv009820_retaildb.employees values(1,'jeswanth',1,10000)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4179aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th><th>dept_id</th><th>salary</th></tr>\n",
       "<tr><td>1</td><td>jeswanth</td><td>1</td><td>10000</td></tr>\n",
       "<tr><td>2</td><td>naidu</td><td>1</td><td>20000</td></tr>\n",
       "<tr><td>3</td><td>sai pavan</td><td>1</td><td>30000</td></tr>\n",
       "<tr><td>4</td><td>nagesh</td><td>2</td><td>15000</td></tr>\n",
       "<tr><td>5</td><td>rajesh</td><td>2</td><td>16000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+---------+-------+------+\n",
       "|emp_id| emp_name|dept_id|salary|\n",
       "+------+---------+-------+------+\n",
       "|     1| jeswanth|      1| 10000|\n",
       "|     2|    naidu|      1| 20000|\n",
       "|     3|sai pavan|      1| 30000|\n",
       "|     4|   nagesh|      2| 15000|\n",
       "|     5|   rajesh|      2| 16000|\n",
       "+------+---------+-------+------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from itv009820_retaildb.employees order by emp_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addc758a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th><th>dept_id</th><th>salary</th><th>dept_id</th><th>dept_name</th></tr>\n",
       "<tr><td>3</td><td>sai pavan</td><td>1</td><td>30000</td><td>1</td><td>CSE</td></tr>\n",
       "<tr><td>5</td><td>rajesh</td><td>2</td><td>16000</td><td>2</td><td>ECE</td></tr>\n",
       "<tr><td>4</td><td>nagesh</td><td>2</td><td>15000</td><td>2</td><td>ECE</td></tr>\n",
       "<tr><td>1</td><td>jeswanth</td><td>1</td><td>10000</td><td>1</td><td>CSE</td></tr>\n",
       "<tr><td>2</td><td>naidu</td><td>1</td><td>20000</td><td>1</td><td>CSE</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+---------+-------+------+-------+---------+\n",
       "|emp_id| emp_name|dept_id|salary|dept_id|dept_name|\n",
       "+------+---------+-------+------+-------+---------+\n",
       "|     3|sai pavan|      1| 30000|      1|      CSE|\n",
       "|     5|   rajesh|      2| 16000|      2|      ECE|\n",
       "|     4|   nagesh|      2| 15000|      2|      ECE|\n",
       "|     1| jeswanth|      1| 10000|      1|      CSE|\n",
       "|     2|    naidu|      1| 20000|      1|      CSE|\n",
       "+------+---------+-------+------+-------+---------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from itv009820_retaildb.employees e inner join itv009820_retaildb.departments d on e.dept_id==d.dept_id \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3467939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df=spark.read.table(\"itv009820_retaildb.employees\")\n",
    "dept_df=spark.read.table(\"itv009820_retaildb.departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a73257d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|dept_id|dept_name|\n",
      "+-------+---------+\n",
      "|      1|      CSE|\n",
      "|      2|      ECE|\n",
      "|      3|      EEE|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9608552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+------+-------+---------+\n",
      "|emp_id| emp_name|dept_id|salary|dept_id|dept_name|\n",
      "+------+---------+-------+------+-------+---------+\n",
      "|     1| jeswanth|      1| 10000|      1|      CSE|\n",
      "|     2|    naidu|      1| 20000|      1|      CSE|\n",
      "|     3|sai pavan|      1| 30000|      1|      CSE|\n",
      "|     4|   nagesh|      2| 15000|      2|      ECE|\n",
      "|     5|   rajesh|      2| 16000|      2|      ECE|\n",
      "+------+---------+-------+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df=emp_df.join(dept_df,emp_df.dept_id==dept_df.dept_id,\"Inner\").orderBy(\"emp_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2df65b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+------+-------+---------+\n",
      "|emp_id| emp_name|dept_id|salary|dept_id|dept_name|\n",
      "+------+---------+-------+------+-------+---------+\n",
      "|     3|sai pavan|      1| 30000|      1|      CSE|\n",
      "|     5|   rajesh|      2| 16000|      2|      ECE|\n",
      "|     4|   nagesh|      2| 15000|      2|      ECE|\n",
      "|     1| jeswanth|      1| 10000|      1|      CSE|\n",
      "|     2|    naidu|      1| 20000|      1|      CSE|\n",
      "+------+---------+-------+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f722d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales=[(1,'2024-07-23',10,4000),(2,'2024-07-25',5,1000),(3,'2024-08-23',3,2000),(5,'2024-07-30',8,6000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d29fc317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '2024-07-23', 10, 4000), (2, '2024-07-25', 5, 1000), (3, '2024-08-23', 3, 2000), (5, '2024-07-30', 8, 6000)]\n"
     ]
    }
   ],
   "source": [
    "print(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9dbbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_df=spark.createDataFrame(sales,[\"product_id\",\"sales_date\",\"quantity\",\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9bbdfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+\n",
      "|product_id|sales_date|quantity|price|\n",
      "+----------+----------+--------+-----+\n",
      "|         1|2024-07-23|      10| 4000|\n",
      "|         2|2024-07-25|       5| 1000|\n",
      "|         3|2024-08-23|       3| 2000|\n",
      "|         5|2024-07-30|       8| 6000|\n",
      "+----------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sale_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5592d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cba538d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales=sale_df.withColumn(\"total_sale\",expr(\"quantity*price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc657173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+----------+\n",
      "|product_id|sales_date|quantity|price|total_sale|\n",
      "+----------+----------+--------+-----+----------+\n",
      "|         1|2024-07-23|      10| 4000|     40000|\n",
      "|         2|2024-07-25|       5| 1000|      5000|\n",
      "|         3|2024-08-23|       3| 2000|      6000|\n",
      "|         5|2024-07-30|       8| 6000|     48000|\n",
      "+----------+----------+--------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f28f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_data = [\n",
    "    (1, 101, \"2024-07-01\", 250.00),\n",
    "    (2, 102, \"2024-07-01\", 300.00),\n",
    "    (3, 101, \"2024-07-02\", 150.00),\n",
    "    (4, 103, \"2024-07-02\", 450.00),\n",
    "    (5, 104, \"2024-07-03\", 200.00),\n",
    "    (6, 102, \"2024-07-03\", 350.00),\n",
    "    (7, 101, \"2024-07-04\", 400.00),\n",
    "    (8, 105, \"2024-07-04\", 500.00),\n",
    "    (9, 103, \"2024-07-05\", 300.00),\n",
    "    (10, 104, \"2024-07-05\", 100.00),\n",
    "    (11, 105, \"2024-07-06\", 450.00),\n",
    "    (12, 102, \"2024-07-06\", 250.00)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"order_date\", \"amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f1b90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+\n",
      "|order_id|customer_id|order_date|amount|\n",
      "+--------+-----------+----------+------+\n",
      "|       1|        101|2024-07-01| 250.0|\n",
      "|       2|        102|2024-07-01| 300.0|\n",
      "|       3|        101|2024-07-02| 150.0|\n",
      "|       4|        103|2024-07-02| 450.0|\n",
      "|       5|        104|2024-07-03| 200.0|\n",
      "|       6|        102|2024-07-03| 350.0|\n",
      "|       7|        101|2024-07-04| 400.0|\n",
      "|       8|        105|2024-07-04| 500.0|\n",
      "|       9|        103|2024-07-05| 300.0|\n",
      "|      10|        104|2024-07-05| 100.0|\n",
      "|      11|        105|2024-07-06| 450.0|\n",
      "|      12|        102|2024-07-06| 250.0|\n",
      "+--------+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279a0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74845cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df=spark.sql(\"select customer_id from orders group by customer_id having count(*)>2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f95612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|        101|\n",
      "|        102|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "438f0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,countDistinct,col,sum as _sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d0a4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df1=orders_df.groupby(\"customer_id\").agg(count(\"*\")).alias(\"order_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29331f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dist_df1=orders_df.groupby(\"customer_id\").agg(countDistinct(\"*\")).alias(\"order_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f38c1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n",
      "|customer_id|count(unresolvedstar())|\n",
      "+-----------+-----------------------+\n",
      "|        103|                      2|\n",
      "|        104|                      2|\n",
      "|        105|                      2|\n",
      "|        101|                      3|\n",
      "|        102|                      3|\n",
      "+-----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_dist_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fe9170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|customer_id|count(1)|\n",
      "+-----------+--------+\n",
      "|        101|       3|\n",
      "|        102|       3|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df1.filter(col(\"count(1)\")>2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2fd9efe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-db3a9f89d31d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-db3a9f89d31d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Question 4: PySpark - Window Functions\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "Question 4: PySpark - Window Functions\n",
    "Scenario: You have a DataFrame transactions with columns customer_id, transaction_date, and amount.\n",
    "\n",
    "Task: Write a PySpark code to calculate the running total of the transaction amounts for each customer.\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f247dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_data = [\n",
    "    (101, \"2024-07-01\", 100.00),\n",
    "    (102, \"2024-07-01\", 200.00),\n",
    "    (101, \"2024-07-02\", 150.00),\n",
    "    (103, \"2024-07-02\", 250.00),\n",
    "    (101, \"2024-07-03\", 200.00),\n",
    "    (102, \"2024-07-03\", 300.00),\n",
    "    (104, \"2024-07-03\", 400.00),\n",
    "    (103, \"2024-07-04\", 150.00),\n",
    "    (104, \"2024-07-04\", 100.00),\n",
    "    (101, \"2024-07-05\", 250.00),\n",
    "    (102, \"2024-07-05\", 150.00),\n",
    "    (103, \"2024-07-06\", 200.00),\n",
    "    (104, \"2024-07-06\", 300.00),\n",
    "    (105, \"2024-07-06\", 500.00)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622abf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df=spark.createDataFrame(transactions_data,[\"customer_id\",\"transaction_date\",\"amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d5b7c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+\n",
      "|customer_id|transaction_date|amount|\n",
      "+-----------+----------------+------+\n",
      "|        101|      2024-07-01| 100.0|\n",
      "|        102|      2024-07-01| 200.0|\n",
      "|        101|      2024-07-02| 150.0|\n",
      "|        103|      2024-07-02| 250.0|\n",
      "|        101|      2024-07-03| 200.0|\n",
      "|        102|      2024-07-03| 300.0|\n",
      "|        104|      2024-07-03| 400.0|\n",
      "|        103|      2024-07-04| 150.0|\n",
      "|        104|      2024-07-04| 100.0|\n",
      "|        101|      2024-07-05| 250.0|\n",
      "|        102|      2024-07-05| 150.0|\n",
      "|        103|      2024-07-06| 200.0|\n",
      "|        104|      2024-07-06| 300.0|\n",
      "|        105|      2024-07-06| 500.0|\n",
      "+-----------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d02dc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a7382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec=Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "528dbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df=transaction_df.withColumn(\"transaction_amount\",_sum(\"amount\").over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8879094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- transaction_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73e5ba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+------------------+\n",
      "|customer_id|transaction_date|amount|transaction_amount|\n",
      "+-----------+----------------+------+------------------+\n",
      "|        103|      2024-07-02| 250.0|             250.0|\n",
      "|        103|      2024-07-04| 150.0|             400.0|\n",
      "|        103|      2024-07-06| 200.0|             600.0|\n",
      "|        104|      2024-07-03| 400.0|             400.0|\n",
      "|        104|      2024-07-04| 100.0|             500.0|\n",
      "|        104|      2024-07-06| 300.0|             800.0|\n",
      "|        105|      2024-07-06| 500.0|             500.0|\n",
      "|        101|      2024-07-01| 100.0|             100.0|\n",
      "|        101|      2024-07-02| 150.0|             250.0|\n",
      "|        101|      2024-07-03| 200.0|             450.0|\n",
      "|        101|      2024-07-05| 250.0|             700.0|\n",
      "|        102|      2024-07-01| 200.0|             200.0|\n",
      "|        102|      2024-07-03| 300.0|             500.0|\n",
      "|        102|      2024-07-05| 150.0|             650.0|\n",
      "+-----------+----------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec84596",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-751ebeb8e64c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-751ebeb8e64c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Question 8: SQL - Aggregations\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Question 8: SQL - Aggregations\n",
    "Scenario: You have a table sales with columns product_id, sale_date, units_sold, and revenue.\n",
    "\n",
    "Task: Write a SQL query to find the total revenue and total units sold for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81791c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE if not exists itv009820_retaildb.sales (product_id INT,sale_date DATE,units_sold INT,revenue DECIMAL(10, 2));\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39555a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO itv009820_retaildb.sales VALUES \n",
    "(1, DATE '2024-01-05', 10, 100.00),\n",
    "(2, DATE '2024-01-15', 20, 300.00),\n",
    "(3, DATE '2024-02-20', 15, 150.00),\n",
    "(1, DATE '2024-02-25', 30, 300.00),\n",
    "(2, DATE '2024-03-05', 25, 250.00),\n",
    "(3, DATE '2024-03-10', 5, 50.00),\n",
    "(1, DATE '2024-03-15', 35, 350.00),\n",
    "(2, DATE '2024-04-05', 40, 400.00),\n",
    "(3, DATE '2024-04-20', 10, 100.00),\n",
    "(1, DATE '2024-04-25', 50, 500.00);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85a62a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>product_id</th><th>sale_date</th><th>units_sold</th><th>revenue</th></tr>\n",
       "<tr><td>1</td><td>2024-01-05</td><td>10</td><td>100.00</td></tr>\n",
       "<tr><td>2</td><td>2024-01-15</td><td>20</td><td>300.00</td></tr>\n",
       "<tr><td>3</td><td>2024-02-20</td><td>15</td><td>150.00</td></tr>\n",
       "<tr><td>1</td><td>2024-02-25</td><td>30</td><td>300.00</td></tr>\n",
       "<tr><td>2</td><td>2024-03-05</td><td>25</td><td>250.00</td></tr>\n",
       "<tr><td>1</td><td>2024-01-05</td><td>10</td><td>100.00</td></tr>\n",
       "<tr><td>2</td><td>2024-01-15</td><td>20</td><td>300.00</td></tr>\n",
       "<tr><td>3</td><td>2024-02-20</td><td>15</td><td>150.00</td></tr>\n",
       "<tr><td>1</td><td>2024-02-25</td><td>30</td><td>300.00</td></tr>\n",
       "<tr><td>2</td><td>2024-03-05</td><td>25</td><td>250.00</td></tr>\n",
       "<tr><td>3</td><td>2024-03-10</td><td>5</td><td>50.00</td></tr>\n",
       "<tr><td>1</td><td>2024-03-15</td><td>35</td><td>350.00</td></tr>\n",
       "<tr><td>2</td><td>2024-04-05</td><td>40</td><td>400.00</td></tr>\n",
       "<tr><td>3</td><td>2024-04-20</td><td>10</td><td>100.00</td></tr>\n",
       "<tr><td>1</td><td>2024-04-25</td><td>50</td><td>500.00</td></tr>\n",
       "<tr><td>3</td><td>2024-03-10</td><td>5</td><td>50.00</td></tr>\n",
       "<tr><td>1</td><td>2024-03-15</td><td>35</td><td>350.00</td></tr>\n",
       "<tr><td>2</td><td>2024-04-05</td><td>40</td><td>400.00</td></tr>\n",
       "<tr><td>3</td><td>2024-04-20</td><td>10</td><td>100.00</td></tr>\n",
       "<tr><td>1</td><td>2024-04-25</td><td>50</td><td>500.00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------+----------+-------+\n",
       "|product_id| sale_date|units_sold|revenue|\n",
       "+----------+----------+----------+-------+\n",
       "|         1|2024-01-05|        10| 100.00|\n",
       "|         2|2024-01-15|        20| 300.00|\n",
       "|         3|2024-02-20|        15| 150.00|\n",
       "|         1|2024-02-25|        30| 300.00|\n",
       "|         2|2024-03-05|        25| 250.00|\n",
       "|         1|2024-01-05|        10| 100.00|\n",
       "|         2|2024-01-15|        20| 300.00|\n",
       "|         3|2024-02-20|        15| 150.00|\n",
       "|         1|2024-02-25|        30| 300.00|\n",
       "|         2|2024-03-05|        25| 250.00|\n",
       "|         3|2024-03-10|         5|  50.00|\n",
       "|         1|2024-03-15|        35| 350.00|\n",
       "|         2|2024-04-05|        40| 400.00|\n",
       "|         3|2024-04-20|        10| 100.00|\n",
       "|         1|2024-04-25|        50| 500.00|\n",
       "|         3|2024-03-10|         5|  50.00|\n",
       "|         1|2024-03-15|        35| 350.00|\n",
       "|         2|2024-04-05|        40| 400.00|\n",
       "|         3|2024-04-20|        10| 100.00|\n",
       "|         1|2024-04-25|        50| 500.00|\n",
       "+----------+----------+----------+-------+"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from itv009820_retaildb.sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0905991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retaildb_df=spark.read.table(\"itv009820_retaildb.sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d42d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b1b23f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df = retaildb_df.select(date_format('sale_date', 'MM-dd-yyyy').alias('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90b7f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retaildb_df=retaildb_df.withColumn(\"date\",date_format('sale_date','MM-yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "640436cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df=retaildb_df.groupby(\"date\").agg(sum(\"units_sold\").alias(\"total_units_sold\"),sum(\"revenue\").alias(\"total_revenue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c129addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-------------+\n",
      "|   date|total_units_sold|total_revenue|\n",
      "+-------+----------------+-------------+\n",
      "|03-2024|             130|      1300.00|\n",
      "|01-2024|              60|       800.00|\n",
      "|04-2024|             200|      2000.00|\n",
      "|02-2024|              90|       900.00|\n",
      "+-------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
